{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring The Telenor Telco Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "import cufflinks as cf\n",
    "import glob\n",
    "import sys\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "cf.go_offline()\n",
    "init_notebook_mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../Data/Data_DK/radio_kpis_sample.csv'\n",
    "#data_path = '../../Hackathon2021_Telenor/hackathon_kpis_anonymised.csv'\n",
    "df = pd.read_csv(data_path, sep=\";\")\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some info\n",
    "From below we see that we have 1048575 rows of data, each with 24 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print number of nans for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nan_values = df.isna().sum()\n",
    "print(nan_values)\n",
    "nan_columns = nan_values[nan_values > 0].index.tolist()\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing data for a TEST cell \n",
    "df = df[df['cell_name'] != 'J5002D11_TEST']\n",
    "\n",
    "# Converting a OBJECT column to FLOAT\n",
    "df['thp_nom_tt_kpi'].replace('0,00E+00', 0.0, inplace=True)\n",
    "df['thp_denom_tt_kpi'].replace('0,00E+00', 0.0, inplace=True)\n",
    "\n",
    "df['thp_nom_tt_kpi'] = df['thp_nom_tt_kpi'].astype(float)\n",
    "df['thp_denom_tt_kpi'] = df['thp_denom_tt_kpi'].astype(float)\n",
    "\n",
    "# Change sector values and carrier\n",
    "df['sector'] = df['sector'].apply(lambda l: int(l[-1]))\n",
    "df['cell_name'] = df['cell_name'].apply(lambda l: int(l[-2]))\n",
    "df = df.rename(columns={'cell_name': 'carrier'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['timestamp'] = df['period_start_time'].apply(lambda l: str(l[:10]) + ' ' + str(l[11:19]))\n",
    "df['day_partition_key'] = df['day_partition_key'].astype(str)\n",
    "df['cell_hour'] = df['cell_hour'].astype(str)\n",
    "#df['cell_hour'] = df['cell_hour'].apply(lambda l: '0' + l if len(l) == 1 else l)\n",
    "\n",
    "df['timestamp'] = df['day_partition_key'] + df['cell_hour']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting data after timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H')\n",
    "df = df.sort_values(by=['timestamp'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the resolution \n",
    "From below we see that it is hourly, and only once is there a gap of more than an hour (2020-03-29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(['timestamp'])\n",
    "\n",
    "for timestamp, group in grouped_df:\n",
    "    t = timestamp\n",
    "    break\n",
    "\n",
    "for timestamp, group in grouped_df:\n",
    "    next_t = timestamp\n",
    "    td = next_t - t\n",
    "    td_mins = int(td.total_seconds() / 60)\n",
    "    if td_mins != 60:\n",
    "        print(f'Time between {t} and {timestamp} is not 1 hour, but {td_mins/60} hours.')\n",
    "    t = next_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking number of cells that has data present for each timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the number of cells present varies a lot and flunctuates around ~100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "num_cells_at_time = []\n",
    "for timestamp, group in grouped_df:\n",
    "    timestamps.append(timestamp)\n",
    "    num_cells_at_time.append(len(group))\n",
    "\n",
    "num_cell_df = pd.DataFrame({'timestamp': timestamps, 'num_cells': num_cells_at_time})\n",
    "fig = px.line(num_cell_df, x=\"timestamp\", y='num_cells', title='Number of cells for each timestamp')\n",
    "fig.show()\n",
    "\n",
    "plt.boxplot(num_cells_at_time);\n",
    "plt.title('Boxplot with number of cells for each timestamp')\n",
    "plt.ylabel('# of cells')\n",
    "plt.xticks([], [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating cells within same sector\n",
    "\n",
    "### For Hackaton data\n",
    "The cell_name is of the form 'XX_ija', where:\n",
    "\n",
    "- XX in {00,01,02,..,30} denotes the site the cell belongs to;\n",
    "- i in {1,2,3} denotes the sector the cell belongs to;\n",
    "- j in {1,2,...} denotes the carrier;\n",
    "- a in {'Z','X','Y','W','V','R','Q','P'} denotes the technology and frequency of the cell based on the table below.\n",
    "\n",
    "### For Original data\n",
    "The cell_name is of the form: '{1char}{4digits}{1char}{2chars}', where:\n",
    "\n",
    "- 1char is Area\n",
    "- 4digits is range per Area\n",
    "- 1char is System (frequency/technology)\n",
    "- 2chars: 1st digit is carrier layer, starting from 1. 2nd digit is sector, range 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_original_data = True\n",
    "df2 = df.copy()\n",
    "\n",
    "if not is_original_data:    \n",
    "    df2['site'] = df2['cell_name'].apply(lambda s: int(s.split('_')[0]))\n",
    "    df2['sector'] = df2['cell_name'].apply(lambda s: int(s.split('_')[1][0]))\n",
    "    df2['carrier'] = df2['cell_name'].apply(lambda s: int(s.split('_')[1][1]))\n",
    "    df2['tech'] = df2['cell_name'].apply(lambda s: s.split('_')[1][2])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NaN using mean or median of column within same sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = df2.copy()\n",
    "for c in nan_columns:\n",
    "    if is_numeric_dtype(agg_df[c]):\n",
    "        agg_df[c] = agg_df[c].fillna(agg_df.groupby(['timestamp', 'site', 'sector'])[c].transform('median'))\n",
    "        agg_df[c] = agg_df[c].fillna(agg_df.groupby(['timestamp', 'site'])[c].transform('median'))\n",
    "        agg_df[c] = agg_df[c].fillna(agg_df.groupby(['site', 'sector'])[c].transform('median'))\n",
    "        agg_df[c] = agg_df[c].fillna(agg_df.groupby(['site'])[c].transform('median'))\n",
    "        agg_df[c] = agg_df[c].fillna(agg_df[c].median())\n",
    "\n",
    "print(agg_df.isna().sum())\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now aggregate across each sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To aggregate columns differently, pass this to the agg-function\n",
    "#aggregate_methods = {\n",
    " #   'avail_period_duration': 'mean',\n",
    "  #  'unavail_unplan_nom': 'mean',\n",
    "   # 'unavail_unplan_denom': 'mean',\n",
    "    #'unavail_total_nom': 'mean',\n",
    "    #'unavail_total_denom': 'mean',\n",
    "    #'bandwidth': 'mean'\n",
    "#}\n",
    "\n",
    "agg_df = agg_df.groupby(['timestamp', 'site', 'sector']).agg('mean').reset_index()\n",
    "# Remove irrelevant columns \n",
    "agg_df.drop(columns=['gid', 'carrier'], inplace=True)\n",
    "agg_df[(agg_df['site'] == 'J1824') & (agg_df['sector'] == 1)].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grouped_df = agg_df.groupby(['site', 'sector'])\n",
    "\n",
    "for (site, sector), group in grouped_df:\n",
    "    count = 0\n",
    "    group = group.sort_values(by=['timestamp'])\n",
    "    t = group.iloc[0, 0]\n",
    "    for i in range(1, group.shape[0]):\n",
    "        next_t = group.iloc[i, 0]\n",
    "        td = next_t - t\n",
    "        td_mins = int(td.total_seconds() / 60)\n",
    "        if td_mins != 60:\n",
    "            count += 1\n",
    "            # print(f'Time between {t} and {next_t} is not 1 hour, but {td_mins/60} hours.')\n",
    "        t = next_t\n",
    "    print(f'{site}-{sector} has {count} (out of {group.shape[0]}) samples with more than 1 hour from last timestamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped_agg_df = agg_df.groupby(['timestamp'])\n",
    "timestamps = []\n",
    "num_sectors_at_time = []\n",
    "for timestamp, group in grouped_agg_df:\n",
    "    timestamps.append(timestamp)\n",
    "    num_sectors_at_time.append(len(group))\n",
    "\n",
    "num_sector_df = pd.DataFrame({'timestamp': timestamps, 'num_sectors': num_sectors_at_time})\n",
    "fig = px.line(num_sector_df, x=\"timestamp\", y='num_sectors', hover_data={\"timestamp\": \"|%B %d. %H:%M, %Y\"}, title='Number of sectors for each timestamp')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "plt.boxplot(num_sectors_at_time);\n",
    "plt.title('Boxplot with number of sectors for each timestamp')\n",
    "plt.ylabel('# of sectors')\n",
    "plt.xticks([], [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the number of cells in each sector, for each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(70, 10))\n",
    "ax = sns.countplot(x=\"site\", hue=\"sector\", data=agg_df)\n",
    "plt.legend(loc='upper right', fontsize=40)\n",
    "ax.set_ylabel('# of cells', fontsize=50)\n",
    "ax.xaxis.label.set_size(50)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.title('Number of cells within each sector, for each site', fontsize=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing data for one of the sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sector(df, site, sector, column):\n",
    "    sector_df = df[(df['site'] == site) & (df['sector'] == sector)]\n",
    "    fig = px.line(sector_df, x=\"timestamp\", y=column)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sector(agg_df, 'J0847', 1, 'mcdr_denom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset \n",
    "In **agg_df**, each row corresponds to aggregated data from all cells within a sector, at a specific timestamp. However, not all sectors do have at least one active cell at all timestamps. If a sector contains no active cells for a timestamp, then a row for that sector at that timestamp does not exist. To make the resolution (every hour) and shape (there is a row for all sectors for all timestamps) consistent, we must insert rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_values = {\n",
    "    'avail_period_duration': 60,\n",
    "    'unavail_unplan_nom': 0,\n",
    "    'unavail_unplan_denom': 60,\n",
    "    'unavail_total_nom': 0,\n",
    "    'unavail_total_denom': 60,\n",
    "    'bandwidth': 0,\n",
    "    'mcdr_denom': 0, \n",
    "    'mcdr_nom_s': 0,\n",
    "    'mcdr_nom_d': 0,\n",
    "    'msdr_denom': 0,\n",
    "    'msdr_nom_s': 0,\n",
    "    'msdr_nom_d': 0,\n",
    "    'thp_denom_tt_kpi': 0,\n",
    "    'thp_nom_tt_kpi': 0,\n",
    "    'ho_denom': 0,\n",
    "    'ho_nom': 0\n",
    "}\n",
    "\n",
    "default_values_list = [default_values[c] for c in agg_df.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_present_set = set([tuple(x) for x in agg_df[['timestamp', 'site', 'sector']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_start = agg_df.head(1)['timestamp'].iloc[0]\n",
    "datetime_end = agg_df.tail(1)['timestamp'].iloc[0]\n",
    "\n",
    "all_timestamps = pd.date_range(datetime_start, datetime_end, freq=\"H\")\n",
    "all_sites = agg_df['site'].unique()\n",
    "all_sectors = agg_df['sector'].unique()\n",
    "\n",
    "rows_to_add = []\n",
    "for timestamp in tqdm(all_timestamps):\n",
    "    for site in all_sites:\n",
    "        for sector in all_sectors:\n",
    "            if (timestamp, site, sector) not in is_present_set:\n",
    "                row_insert = [timestamp, site, sector]\n",
    "                row_insert.extend(default_values_list)\n",
    "                rows_to_add.append(row_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_add_df = pd.DataFrame(rows_to_add, columns=agg_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df_filled = agg_df.append(rows_to_add_df)\n",
    "agg_df_filled = agg_df_filled.sort_values(by=['timestamp', 'site', 'sector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agg_df_filled.shape)\n",
    "agg_df_filled.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that new dataframe now has constant number of sectors for each timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_agg_df = agg_df_filled.groupby(['timestamp'])\n",
    "timestamps = []\n",
    "num_sectors_at_time = []\n",
    "for timestamp, group in grouped_agg_df:\n",
    "    timestamps.append(timestamp)\n",
    "    num_sectors_at_time.append(len(group))\n",
    "\n",
    "num_sector_df = pd.DataFrame({'timestamp': timestamps, 'num_sectors': num_sectors_at_time})\n",
    "fig = px.line(num_sector_df, x=\"timestamp\", y='num_sectors', hover_data={\"timestamp\": \"|%B %d. %H:%M, %Y\"}, title='Number of sectors for each timestamp')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize column again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_sector(agg_df_filled, 'J0847', 1, 'mcdr_denom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we group the data to get a series of graph signals with shape: (# of timestamps, # of sectors, # of features)\n",
    "\n",
    "<img src=\"https://i.imgur.com/1izToWi.png\" width=750 height=750 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_agg_df = agg_df_filled.groupby(['timestamp'])\n",
    "graph_signals = []\n",
    "\n",
    "for timestamp, group in tqdm(grouped_agg_df):\n",
    "    graph_signals.append(group)\n",
    "\n",
    "graph_signals = np.array(graph_signals)\n",
    "\n",
    "metadata = {\n",
    "    'start': str(agg_df_filled.head(1)['timestamp'].iloc[0]),\n",
    "    'end': str(agg_df_filled.tail(1)['timestamp'].iloc[0]),\n",
    "    'resolution_minutes': (list(grouped_agg_df)[1][0] - list(grouped_agg_df)[0][0]).seconds / 60,\n",
    "    'columns': agg_df_filled.columns[1:].tolist(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata)\n",
    "print(graph_signals.shape)\n",
    "\n",
    "with open('../datasets/telenor/metadata.txt', 'w') as file:\n",
    "    json.dump(metadata, file, indent=2)\n",
    "    \n",
    "np.save('../datasets/telenor/graph_signals.npy', graph_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_df_processed = sector_df.copy().drop(columns=['timestamp', 'site', 'sector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Save data for each sector as separate numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sector_data(df, site, sector, test_split=0.3):\n",
    "    data = df.copy()\n",
    "    sector_data = data[(data['site'] == site) & (data['sector'] == sector)]\n",
    "    sector_data = sector_data.sort_values(by=['timestamp'])\n",
    "    sector_data_processed = sector_data.drop(columns=['timestamp', 'site', 'sector', 'carrier'])\n",
    "    train_idx_stop = int(sector_data_processed.shape[0] * (1-test_split))\n",
    "    train = sector_data_processed.iloc[:train_idx_stop, :]\n",
    "    test = sector_data_processed.iloc[train_idx_stop:, :]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = False\n",
    "if save_data:\n",
    "    train_folder = '../datasets/telenor/train'\n",
    "    test_folder = '../datasets/telenor/test'\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "    labeled_anomalies = {'chan_id': [],\n",
    "                         'anomaly_sequences': [],\n",
    "                         'class': [],\n",
    "                         'num_values': []}\n",
    "\n",
    "    site_sector_combinations = agg_df[['site', 'sector']].value_counts().index.tolist()\n",
    "    for site, sector in sorted(site_sector_combinations):\n",
    "        train, test = get_sector_data(agg_df, site, sector, test_split=0.3)\n",
    "        np.save(f'{train_folder}/{site}-{sector}.npy', train)\n",
    "        np.save(f'{test_folder}/{site}-{sector}.npy', test)\n",
    "\n",
    "        labeled_anomalies['chan_id'].append(f'{site}-{sector}')\n",
    "        labeled_anomalies['anomaly_sequences'].append([])\n",
    "        labeled_anomalies['class'].append('')\n",
    "        labeled_anomalies['num_values'].append(test.shape[0])\n",
    "\n",
    "    labeled_anomalies_df = pd.DataFrame(labeled_anomalies)\n",
    "    labeled_anomalies_df.to_csv('../datasets/telenor/labeled_anomalies.csv')\n",
    "    \n",
    "    train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
